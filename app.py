# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ì„¤ì •
import streamlit as st
import numpy as np
import pandas as pd
import re
import base64
import streamlit.components.v1 as components
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from PIL import Image
from io import BytesIO
from sklearn.impute import KNNImputer
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.preprocessing import RobustScaler
from sklearn.pipeline import make_pipeline
 
st.markdown(
    """
    <style>
    /* ìƒë‹¨ 'Made with Streamlit' ë¡œê³  ìˆ¨ê¹€ */
    header {visibility: hidden;}
    
    /* í•˜ë‹¨ footer ìˆ¨ê¹€ (ë²„íŠ¼ í¬í•¨) */
    footer {visibility: hidden;}
    
    /* íŠ¹ì • í´ë˜ìŠ¤ëª… ìš”ì†Œ ìˆ¨ê¹€ */
    ._profileContainer_gzau3_53 {visibility: hidden;}

    /* íŠ¹ì • í´ë˜ìŠ¤ëª… ìš”ì†Œ ìˆ¨ê¹€ */
    ._container_gzau3_1 _viewerBadge_nim44_23 {visibility: hidden;}

    .st-emotion-cache-1w723zb {padding: 0 !important;}
    </style>
    """,
    unsafe_allow_html=True
)

# í°íŠ¸ ì„¤ì •
font_path = "./NanumGothic.ttf"  # ë˜ëŠ” "./fonts/NanumGothic.ttf"
fontprop = fm.FontProperties(fname=font_path)
plt.rcParams['font.family'] = fontprop.get_name()
plt.rcParams['axes.unicode_minus'] = False
 
# 2. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
@st.cache_data
def load_data():
    df = pd.read_csv("data_logistics.csv")
    df = df.rename(columns={"í˜„ì¥ëª…": "í”„ë¡œì íŠ¸ëª…"})
    return df

df_data = load_data()

# 4. ë³´ì¡° í•¨ìˆ˜ë“¤

def detect_outlier_floors(floor_count):
    if floor_count >= 50:
        st.warning("âš ï¸ ì…ë ¥í•˜ì‹  ì¸µìˆ˜ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ë†’ìŠµë‹ˆë‹¤. ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")

def get_realistic_trust_score(r2, std=None, sim_mean=None, sample_n=None, total_data=None):
    # ë°ì´í„° ìˆ˜ì— ë”°ë¥¸ ì‹ ë¢° ê°€ì¤‘ì¹˜ (ìµœëŒ€ 1.0, ìµœì†Œ 0.5)
    if total_data is not None and total_data < 300:
        data_weight = 0.5 + 0.5 * (total_data / 300)
    else:
        data_weight = 1.0

    score = 0

    # 1. ëª¨ë¸ ì„¤ëª…ë ¥ ê¸°ë°˜ (ìµœëŒ€ 50ì )
    score += r2 * 50

    # 2. ìœ ì‚¬ë„ í‰ê·  ê¸°ë°˜ í‰ê°€ (ìµœëŒ€ 30ì )
    if sim_mean is not None:
        if sim_mean >= 80:
            score += 30
        elif sim_mean >= 60:
            score += 25
        elif sim_mean >= 40:
            score += 20
        elif sim_mean >= 20:
            score += 10

    # 3. í‘œì¤€í¸ì°¨ ê¸°ë°˜ ì•ˆì •ì„± í‰ê°€ (ìµœëŒ€ 20ì )
    if std is not None:
        if std <= 3:
            score += 20
        elif std <= 5:
            score += 15
        elif std <= 8:
            score += 10

    # 4. ìœ ì‚¬ í”„ë¡œì íŠ¸ ìˆ˜ ë³´ì •
    if sample_n is not None and sample_n < 3:
        score -= 10

    # ë°ì´í„°ëŸ‰ ê¸°ë°˜ ì‹ ë¢°ë„ ë³´ì •
    score *= data_weight

    # ì ìˆ˜ ì œí•œ
    score = max(5, min(95, score))
    return int(round(score))

@st.cache_data
def knn_impute(df_input, n_neighbors=3):
    numeric_cols = df_input.select_dtypes(include=["float", "int"]).columns
    imputer = KNNImputer(n_neighbors=n_neighbors)
    df_filled = df_input.copy()
    df_filled[numeric_cols] = imputer.fit_transform(df_input[numeric_cols])
    return df_filled

# 5. ìœ ì‚¬ë„ ê¸°ë°˜ ì˜ˆì¸¡ í•¨ìˆ˜ (RandomForest ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš©)
def find_similar_projects_with_categorical_filter(user_row, df, top_k=5):
    numeric_cols = ['ëŒ€ì§€ë©´ì ', 'ê±´ì¶•ë©´ì ', 'ì—°ë©´ì ', 'ì§€í•˜ì¸µìˆ˜', 'ì§€ìƒì¸µìˆ˜', 'í•˜ì—­ê°€ëŠ¥ì¸µìˆ˜', 'ìµœê³ ë†’ì´', 'ì „ì²´í† ê³µëŸ‰']
    structure_cols = [col for col in df.columns if col.startswith('êµ¬ì¡°í˜•ì‹_')]
    coldtype_cols = [col for col in df.columns if col.startswith('ì°½ê³ ìœ í˜•_')]

    input_values = user_row.iloc[0]
    available_numeric = [col for col in numeric_cols if col in input_values.index and pd.notnull(input_values[col])]
    if not available_numeric:
        raise ValueError("ì…ë ¥ê°’ì— ìœ íš¨í•œ ìˆ˜ì¹˜í˜• í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

    selected_coldtypes = [col for col in coldtype_cols if col in user_row.columns and input_values[col] == 1]
    coldtype_mask = df[coldtype_cols].eq(0)
    for col in selected_coldtypes:
        coldtype_mask[col] = df[col].eq(1)
    coldtype_match = coldtype_mask.all(axis=1)
    df_matched = df[coldtype_match].dropna(subset=available_numeric).copy()

    if df_matched.empty:
        raise ValueError("ì…ë ¥í•œ ì°½ê³ ìœ í˜•ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    feature_cols = available_numeric + structure_cols
    df_model = df.dropna(subset=["ì „ì²´ê³µì‚¬ê¸°ê°„"])
    X_model = df_model[feature_cols]
    y_model = df_model["ì „ì²´ê³µì‚¬ê¸°ê°„"]
    rf_model = RandomForestRegressor(n_estimators=100, random_state=42).fit(X_model, y_model)
    feature_importance = rf_model.feature_importances_
    weights = dict(zip(feature_cols, feature_importance))

    scaler = RobustScaler().fit(df_matched[feature_cols])
    df_scaled = scaler.transform(df_matched[feature_cols])
    user_scaled = scaler.transform(user_row[feature_cols])[0]

    def weighted_euclidean(a, b, w_dict, cols):
        return np.sqrt(np.sum([(a[i] - b[i]) ** 2 * w_dict[cols[i]] for i in range(len(cols))]))

    distances = np.array([weighted_euclidean(row, user_scaled, weights, feature_cols) for row in df_scaled])

    # MinMax ì •ê·œí™”ëœ ìœ ì‚¬ë„ ê³„ì‚°
    max_dist = distances.max()
    min_dist = distances.min()
    if max_dist == min_dist:
        # ëª¨ë“  ê±°ë¦¬ê°€ ê°™ìœ¼ë©´ ë™ì¼ ìœ ì‚¬ë„ ë¶€ì—¬
        similarities = np.ones_like(distances)
    else:
        similarities = 1 - ((distances - min_dist) / (max_dist - min_dist))

    df_matched["ìœ ì‚¬ë„ì ìˆ˜"] = similarities * 100  # 0~100 ì ìˆ˜í™”

    top_similar = df_matched.sort_values("ìœ ì‚¬ë„ì ìˆ˜", ascending=False).head(top_k)
    if "ì „ì²´ê³µì‚¬ê¸°ê°„" not in top_similar.columns or top_similar["ì „ì²´ê³µì‚¬ê¸°ê°„"].isnull().all():
        predicted_months = np.nan
    else:
        weighted_sum = (top_similar["ì „ì²´ê³µì‚¬ê¸°ê°„"] * top_similar["ìœ ì‚¬ë„ì ìˆ˜"]).sum()
        sum_weights = top_similar["ìœ ì‚¬ë„ì ìˆ˜"].sum()
        predicted_months = weighted_sum / sum_weights

    return top_similar, round(predicted_months, 1) if not np.isnan(predicted_months) else None

# 6. ì‚¬ìš©ì ì…ë ¥ê°’ íŒŒì‹± í•¨ìˆ˜ ì¶”ê°€
def parse_float(text):
    try:
        return float(text.replace(",", "").strip()) if text else np.nan
    except:
        return np.nan

def parse_int(text):
    try:
        return int(text.strip()) if text else np.nan
    except:
        return np.nan

def generate_explanation(model_type, r2, std, sim_mean, sample_n, trust_score):
    parts = []

    parts.append(f"### {model_type} ê²°ê³¼ í•´ì„")

    # ğŸ”¹ ëª¨ë¸ íŠ¹ì„± ì„¤ëª…
    if model_type == "ì„ í˜•íšŒê·€":
        parts.append("- ì„ í˜•íšŒê·€ ëª¨ë¸ì€ ì…ë ¥ëœ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ê³¼ ê³µì‚¬ê¸°ê°„ ê°„ì˜ ì„ í˜•ì  ê´€ê³„ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.")
    elif model_type == "ëœë¤í¬ë ˆìŠ¤íŠ¸":
        parts.append("- ëœë¤í¬ë ˆìŠ¤íŠ¸ëŠ” ë‹¤ìˆ˜ì˜ ê²°ì • íŠ¸ë¦¬ë¥¼ í™œìš©í•´ ë¹„ì„ í˜• ê´€ê³„ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.")
    elif model_type == "ìœ ì‚¬ í”„ë¡œì íŠ¸ ê¸°ë°˜":
        parts.append("- ìœ ì‚¬ í”„ë¡œì íŠ¸ ê¸°ë°˜ ëª¨ë¸ì€ ì…ë ¥ ì¡°ê±´ê³¼ ìœ ì‚¬í•œ ì‹¤ì œ ì‚¬ë¡€ì˜ ê³µì‚¬ê¸°ê°„ í‰ê· ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡í•©ë‹ˆë‹¤.")

    # ğŸ”¹ ì„¤ëª…ë ¥
    if r2 >= 0.7:
        parts.append(f"- ëª¨ë¸ì˜ ì„¤ëª…ë ¥(RÂ²)ì€ **{r2:.2f}**ë¡œ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤.")
    elif r2 >= 0.4:
        parts.append(f"- ëª¨ë¸ì˜ ì„¤ëª…ë ¥(RÂ²)ì€ **{r2:.2f}**ë¡œ ë³´í†µ ìˆ˜ì¤€ì…ë‹ˆë‹¤.")
    else:
        parts.append(f"- ëª¨ë¸ì˜ ì„¤ëª…ë ¥(RÂ²)ì€ **{r2:.2f}**ë¡œ ë‚®ì•„ ë³€ìˆ˜ë“¤ì´ ì¶©ë¶„íˆ ì„¤ëª…í•˜ì§€ ëª»í•˜ê³  ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # ğŸ”¹ í‘œì¤€í¸ì°¨
    if std <= 3:
        parts.append(f"- ìœ ì‚¬ í”„ë¡œì íŠ¸ ê°„ ê³µì‚¬ê¸°ê°„ í‘œì¤€í¸ì°¨ê°€ **{std:.1f}ê°œì›”**ë¡œ ì•ˆì •ì ì…ë‹ˆë‹¤.")
    elif std <= 6:
        parts.append(f"- ê³µì‚¬ê¸°ê°„ í‘œì¤€í¸ì°¨ê°€ **{std:.1f}ê°œì›”**ë¡œ ì¤‘ê°„ ìˆ˜ì¤€ì…ë‹ˆë‹¤.")
    else:
        parts.append(f"- ê³µì‚¬ê¸°ê°„ í‘œì¤€í¸ì°¨ê°€ **{std:.1f}ê°œì›”**ë¡œ ì˜ˆì¸¡ ì‹ ë¢°ë„ê°€ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # ğŸ”¹ ìœ ì‚¬ë„
    parts.append(f"- í‰ê·  ìœ ì‚¬ë„ëŠ” **{sim_mean:.1f}ì **, ìœ ì‚¬ í”„ë¡œì íŠ¸ ìˆ˜ëŠ” **{sample_n}ê±´**ì…ë‹ˆë‹¤.")

    # ğŸ”¹ í•™ìŠµ ë°ì´í„°ëŸ‰ ê¸°ë°˜ ì„¤ëª…
    if sample_n < 100:
        parts.append(f"- ì´ ëª¨ë¸ì€ ì´ **{sample_n}ê±´**ì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµë˜ì–´ ì¼ë°˜í™”ì— ì œí•œì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        parts.append(f"- ì´ ëª¨ë¸ì€ ì´ **{sample_n}ê±´**ì˜ í•™ìŠµ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡í•˜ì˜€ìŠµë‹ˆë‹¤.")

    # ğŸ”¹ ì‹ ë¢°ë„
    parts.append(f"- ìµœì¢… ì‹ ë¢°ë„ ì ìˆ˜ëŠ” **{trust_score}ì **ì…ë‹ˆë‹¤.")

    return "\n\n".join(parts)



# 7. ì‚¬ìš©ì ì…ë ¥ UI

st.markdown("""
<h2 style='text-align: left;'>ë¬¼ë¥˜ì„¼í„° ê³µì‚¬ê¸°ê°„ ì˜ˆì¸¡ AI</h2>
<p style='text-align: left; font-size: 16px; color: #555555;'>
ì´ AI ëª¨ë¸ì€ ë™ì›ê±´ì„¤ì‚°ì—…ì—ì„œ ì¶•ì í•œ ë¬¼ë¥˜ì„¼í„° ì‹œê³µ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì…ë ¥ëœ í”„ë¡œì íŠ¸ ì¡°ê±´ê³¼ ìœ ì‚¬í•œ ì‚¬ë¡€ë¥¼ ìë™>
ì˜ˆì¸¡ ê²°ê³¼ëŠ” í†µê³„ ê¸°ë°˜ ë³´ì • ê³¼ì •ì„ ê±°ì³ ë³´ë‹¤ í˜„ì‹¤ì ì´ê³  ì‹ ë¢°ë„ ë†’ì€ ì‹ ê·œ ë¬¼ë¥˜ì„¼í„° í”„ë¡œì íŠ¸ì˜ ì˜ˆìƒ ê³µì‚¬ê¸°ê°„ì„ ì œ[>
</p>
""", unsafe_allow_html=True)

with st.expander("í•„ìˆ˜ ì…ë ¥ê°’", expanded=True):
    
    # ğŸ‘‰ í”„ë¡œì íŠ¸ëª… ì…ë ¥ í•„ë“œ ì¶”ê°€
    

    col1, col2, col3 = st.columns(3)
    with col1:
        project_name = st.text_input("í”„ë¡œì íŠ¸ëª…")
        floors_below_str = st.text_input("ì§€í•˜ì¸µìˆ˜", placeholder="ì˜ˆ: 1")
        
    with col2:
        floors_above_str = st.text_input("ì§€ìƒì¸µìˆ˜", placeholder="ì˜ˆ: 4")
        land_area_str = st.text_input("ëŒ€ì§€ë©´ì  (ã¡)", placeholder="ì˜ˆ: 45085")
              
    with col3:
        building_area_str = st.text_input("ê±´ì¶•ë©´ì  (ã¡)", placeholder="ì˜ˆ: 26166")
        total_area_str = st.text_input("ì—°ë©´ì  (ã¡)", placeholder="ì˜ˆ: 70841")

    # ìˆ˜ì¹˜í˜• ë³€í™˜
    land_area = parse_float(land_area_str)
    building_area = parse_float(building_area_str)
    total_area = parse_float(total_area_str)
    floors_above = parse_int(floors_above_str)
    floors_below = parse_int(floors_below_str)

    detect_outlier_floors(floors_above)

    st.divider()
    st.markdown("#### ì°½ê³ ìœ í˜• (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)")
    usage_options = ['ìƒì˜¨', 'ì €ì˜¨', 'ëƒ‰ë™']
    selected_usages = []
    for i, u in enumerate(st.columns(len(usage_options))):
        if u.checkbox(usage_options[i], key=f"usage_{usage_options[i]}"):
            selected_usages.append(usage_options[i])

    st.markdown("#### êµ¬ì¡°í˜•ì‹ (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)")
    structure_options = ['PC', 'RC', 'SRC', 'PEB']
    selected_structures = []
    for i, s in enumerate(st.columns(len(structure_options))):
        if s.checkbox(structure_options[i], key=f"struct_{structure_options[i]}"):
            selected_structures.append(structure_options[i])

with st.expander("ì„ íƒ ì…ë ¥ê°’", expanded=True):
    col4, col5, col6 = st.columns(3)
    with col4:
        excavation_volume_str = st.text_input("ì „ì²´ í† ê³µëŸ‰ (ã¥)", placeholder="ì˜ˆ: 120000")
    with col5:
        dockable_floors_str = st.text_input("í•˜ì—­ ê°€ëŠ¥ì¸µìˆ˜", placeholder="ì˜ˆ: 2")
    with col6:
        height_str = st.text_input("ìµœê³ ë†’ì´ (m)", placeholder="ì˜ˆ: 47.5")

    excavation_volume = parse_float(excavation_volume_str)
    dockable_floors = parse_int(dockable_floors_str)
    height = parse_float(height_str)

# 8. ì˜ˆì¸¡ ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥

if st.button("ì˜ˆì¸¡ ì‹œì‘", use_container_width=True):
    # í•„ìˆ˜ ì…ë ¥ê°’ ëˆ„ë½ ì²´í¬
    required_fields = {
        "ëŒ€ì§€ë©´ì ": land_area,
        "ê±´ì¶•ë©´ì ": building_area,
        "ì—°ë©´ì ": total_area,
        "ì§€ìƒì¸µìˆ˜": floors_above,
        "ì§€í•˜ì¸µìˆ˜": floors_below,
        "ì°½ê³ ìœ í˜•": selected_usages,
        "êµ¬ì¡°í˜•ì‹": selected_structures
    }
    missing_fields = [k for k, v in required_fields.items() if (isinstance(v, list) and not v) or (not isinstance(v, list) and pd.isna(v))]
    if missing_fields:
        st.warning(f"âš ï¸ í•„ìˆ˜ ì…ë ¥ê°’ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(missing_fields)} í•­ëª©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        st.stop()

    # ë…¼ë¦¬ì  ìœ íš¨ì„± ê²€ì‚¬
    logic_errors = []
    if building_area > land_area:
        logic_errors.append("ê±´ì¶•ë©´ì ì´ ëŒ€ì§€ë©´ì ë³´ë‹¤ í´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    if total_area < building_area:
        logic_errors.append("ì—°ë©´ì ì´ ê±´ì¶•ë©´ì ë³´ë‹¤ ì‘ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    if (floors_above + floors_below) == 0 and total_area > 0:
        logic_errors.append("ì¸µìˆ˜ê°€ ì—†ëŠ”ë° ì—°ë©´ì ì´ ì¡´ì¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    if total_area > 0 and building_area > 0 and floors_above > 0:
        estimated_total = building_area * (floors_above + floors_below)
        if total_area > estimated_total * 1.5:
            logic_errors.append("ì—°ë©´ì ì´ ê±´ì¶•ë©´ì Ã—ì¸µìˆ˜ë³´ë‹¤ ì§€ë‚˜ì¹˜ê²Œ í½ë‹ˆë‹¤. ì…ë ¥ê°’ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.")
    if logic_errors:
        st.markdown("### âš ï¸ ì…ë ¥ê°’ ì˜¤ë¥˜ ê°ì§€")
        for err in logic_errors:
            st.error(f"ğŸš« {err}")
        st.stop()

    # ì‚¬ìš©ì ì…ë ¥ê°’ ì •ë¦¬
    user_input = {
        "ëŒ€ì§€ë©´ì ": land_area,
        "ê±´ì¶•ë©´ì ": building_area,
        "ì—°ë©´ì ": total_area,
        "ì§€ìƒì¸µìˆ˜": floors_above,
        "ì§€í•˜ì¸µìˆ˜": floors_below,
        "í•˜ì—­ê°€ëŠ¥ì¸µìˆ˜": dockable_floors if dockable_floors and dockable_floors > 0 else np.nan,
        "ìµœê³ ë†’ì´": height if height and height > 0 else np.nan,
        "ì „ì²´í† ê³µëŸ‰": excavation_volume if excavation_volume and excavation_volume > 0 else np.nan,
    }
    input_df = pd.DataFrame([user_input])
    for s in structure_options:
        input_df[f"êµ¬ì¡°í˜•ì‹_{s}"] = 1 if s in selected_structures else 0
    for u in usage_options:
        input_df[f"ì°½ê³ ìœ í˜•_{u}"] = 1 if u in selected_usages else 0

    # í•™ìŠµ ë°ì´í„° ì¤€ë¹„
    user_corrected = input_df.copy()
    df_model = df_data.dropna(subset=["ì „ì²´ê³µì‚¬ê¸°ê°„"])
    feature_cols = user_corrected.columns.tolist()
    X_all = df_model[feature_cols].copy()
    y_all = df_model["ì „ì²´ê³µì‚¬ê¸°ê°„"]
    X_all = X_all.loc[:, X_all.isna().mean() <= 0.3]
    user_corrected = user_corrected[X_all.columns]

    # KNN ë³´ê°„
    X_all_imputed = knn_impute(X_all)

    # ì„ í˜•íšŒê·€
    model1 = make_pipeline(RobustScaler(), LinearRegression()).fit(X_all_imputed, y_all)
    pred1 = model1.predict(user_corrected)[0]
    r2_1 = model1.score(X_all_imputed, y_all)

    # ëœë¤í¬ë ˆìŠ¤íŠ¸
    model2 = RandomForestRegressor(n_estimators=100, random_state=42).fit(X_all_imputed, y_all)
    pred2 = model2.predict(user_corrected)[0]
    r2_2 = model2.score(X_all_imputed, y_all)

    # ìœ ì‚¬ í”„ë¡œì íŠ¸ ê¸°ë°˜
    similar_df, pred3 = find_similar_projects_with_categorical_filter(user_corrected, df_data)
    sim_std = similar_df["ì „ì²´ê³µì‚¬ê¸°ê°„"].std()
    mean_similarity = similar_df["ìœ ì‚¬ë„ì ìˆ˜"].mean()

    # ì„ í˜•íšŒê·€ ì˜ˆì¸¡ê°’ì˜ std (ì‹¤ì œê°’ê³¼ì˜ ì˜¤ì°¨)
    preds1_all = model1.predict(X_all_imputed)
    std1 = np.std(preds1_all - y_all)

    # ëœë¤í¬ë ˆìŠ¤íŠ¸ ì˜ˆì¸¡ê°’ì˜ std
    preds2_all = model2.predict(X_all_imputed)
    std2 = np.std(preds2_all - y_all)

    # ìœ ì‚¬ë„ ê¸°ë°˜ ëª¨ë¸ì˜ stdëŠ” ê¸°ì¡´ ìœ ì§€
    std3 = sim_std
 
    # ì•™ìƒë¸”
    pred_ensemble = (pred1 * r2_1 + pred2 * r2_2 + pred3 * 1.0) / (r2_1 + r2_2 + 1.0)

    # ì‹ ë¢°ë„ ê²€ì‚¬
    warnings = []
    if sim_std > 6:
        warnings.append("ìœ ì‚¬ í”„ë¡œì íŠ¸ ê°„ ê³µì‚¬ê¸°ê°„ í¸ì°¨ê°€ ì»¤ì„œ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤.")
    if mean_similarity < 25:
        warnings.append("ì…ë ¥ ì¡°ê±´ê³¼ ìœ ì‚¬í•œ í”„ë¡œì íŠ¸ê°€ ì ì–´ ì˜ˆì¸¡ ì •í™•ë„ê°€ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    if r2_1 < 0.3 and r2_2 < 0.3:
        warnings.append("ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤.")
    if warnings:
        st.markdown("### âš ï¸ ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥ì´ ì œí•œë˜ì—ˆìŠµë‹ˆë‹¤.")
        for w in warnings:
            st.error(f"ğŸš« {w}")
        st.info("ì…ë ¥ê°’ì„ ìˆ˜ì •í•˜ê±°ë‚˜ ë³´ì™„í•œ í›„ ë‹¤ì‹œ ì˜ˆì¸¡ì„ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.")
        st.stop()

    
    if project_name:
            # â­ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° ë° ë³„ì  ë³€í™˜
        total_data_count = len(df_model)

        trust_score = get_realistic_trust_score(
            r2=(r2_1 + r2_2) / 2,
            std=sim_std,
            sim_mean=mean_similarity,
            sample_n=len(similar_df),
            total_data=total_data_count  # â† ì—¬ê¸°ì— ì´ ìœ íš¨ ë°ì´í„° ìˆ˜ ì „ë‹¬
        )

        # ì´ ë°ì´í„° ìˆ˜ ì •ì˜
        total_data_count = len(df_model)
     
        train_count_linear = len(X_all_imputed)  # ì„ í˜•íšŒê·€ & ëœë¤í¬ë ˆìŠ¤íŠ¸ ë™ì¼
        train_count_rf = len(X_all_imputed)
        train_count_similar = len(similar_df)


     
        trust1 = get_realistic_trust_score(r2_1, std=std1, sim_mean=mean_similarity, sample_n=len(similar_df), total_data=total_data_count)
        explain1 = generate_explanation("ì„ í˜•íšŒê·€", r2_1, std1, mean_similarity, train_count_linear, trust1)

        trust2 = get_realistic_trust_score(r2_2, std=std2, sim_mean=mean_similarity, sample_n=len(similar_df), total_data=total_data_count)
        explain2 = generate_explanation("ëœë¤í¬ë ˆìŠ¤íŠ¸", r2_2, std2, mean_similarity, train_count_rf, trust2)

        trust3 = get_realistic_trust_score(1.0, std=std3, sim_mean=mean_similarity, sample_n=len(similar_df), total_data=total_data_count)
        explain3 = generate_explanation("ìœ ì‚¬ í”„ë¡œì íŠ¸ ê¸°ë°˜", 1.0, std3, mean_similarity, train_count_similar, trust3)

        

     
        # ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ ëª¨ë¸ ì°¾ê¸°
        model_names = ["ì„ í˜•íšŒê·€", "ëœë¤í¬ë ˆìŠ¤íŠ¸", "ìœ ì‚¬ í”„ë¡œì íŠ¸ ê¸°ë°˜"]
        pred_values = [round(pred1, 1), round(pred2, 1), round(pred3, 1)]
        trust_scores = [trust1, trust2, trust3]
        best_idx = int(np.argmax(trust_scores))
        best_model = model_names[best_idx]
        best_pred = pred_values[best_idx]
        best_trust = trust_scores[best_idx]
     
        # ê²°ê³¼ ì¶œë ¥
        st.markdown(f"""
            <div style="
                background-color: #f4f8fc;
                border: 1px solid #d0e3f1;
                border-radius: 12px;
                padding: 25px;
                margin-top: 20px;
                margin-bottom: 20px;
                box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.05);
            ">
                <div style="text-align: center; font-size: 1.6rem; font-weight: 700; color: #004080;">
                    ğŸ“Œ {project_name} ì‚°ì¶œ ê²°ê³¼
                </div>
                <div style="text-align: center; font-size: 2.4rem; font-weight: bold; color: #00264d; margin-top: 12px;">
                    {best_pred} ê°œì›”
                </div>
                <div style="text-align: center; font-size: 0.95rem; color: #444; margin-top: 10px;">
                    ë³¸ ê²°ê³¼ëŠ” 3ê°œ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ ì¤‘ <strong style="color:#004080;">{best_trust}%</strong>ë¡œ ê°€ì¥ ì‹ ë¢°ë„ê°€ ë†’ì€
                    <strong style="color:#004080;">{best_model}</strong> ê¸°ë°˜ì˜ ì˜ˆì¸¡ ê²°ê³¼ê°’ì…ë‹ˆë‹¤.
                </div>
            </div>
        """, unsafe_allow_html=True)


 
    # ì˜ˆì¸¡ ê²°ê³¼ í‘œ
    st.subheader("â–  ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½")
    final_table = pd.DataFrame({
        "ì˜ˆì¸¡ ë°©ì‹": ["ì„ í˜•íšŒê·€", "ëœë¤í¬ë ˆìŠ¤íŠ¸", "ìœ ì‚¬ í”„ë¡œì íŠ¸ ê¸°ë°˜"],
        "ì˜ˆì¸¡ê°’ (ê°œì›”)": [round(pred1, 1), round(pred2, 1), round(pred3, 1)],
        "ì‹ ë¢°ë„ (%)": [
            get_realistic_trust_score(r2_1, std=std1, sim_mean=mean_similarity, sample_n=len(similar_df), total_data=total_data_count),
            get_realistic_trust_score(r2_2, std=std2, sim_mean=mean_similarity, sample_n=len(similar_df), total_data=total_data_count),
            get_realistic_trust_score(1.0, std=std3, sim_mean=mean_similarity, sample_n=len(similar_df), total_data=total_data_count)
        ]
    })

    st.dataframe(final_table)

    with st.expander(" ì„ í˜•íšŒê·€ ì‹ ë¢°ë„ ì„¤ëª…", expanded=False):
        st.markdown(explain1)

    with st.expander(" ëœë¤í¬ë ˆìŠ¤íŠ¸ ì‹ ë¢°ë„ ì„¤ëª…", expanded=False):
        st.markdown(explain2)

    with st.expander(" ìœ ì‚¬ í”„ë¡œì íŠ¸ ê¸°ë°˜ ì‹ ë¢°ë„ ì„¤ëª…", expanded=False):
        st.markdown(explain3)

    # ğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµí‘œ
    if not similar_df.empty:
        st.subheader("â–  ì˜ˆì¸¡ ê²°ê³¼ ê·¸ë˜í”„")

        fig, ax = plt.subplots(figsize=(10, 5))

        # íˆìŠ¤í† ê·¸ë¨: 1ê°œì›” ë‹¨ìœ„, ë¶€ë“œëŸ¬ìš´ ìŠ¤íƒ€ì¼
        min_month = int(similar_df["ì „ì²´ê³µì‚¬ê¸°ê°„"].min()) - 1
        max_month = int(similar_df["ì „ì²´ê³µì‚¬ê¸°ê°„"].max()) + 1
        bins = np.arange(min_month, max_month + 1)

        ax.hist(
            similar_df["ì „ì²´ê³µì‚¬ê¸°ê°„"],
            bins=bins,
            alpha=0.6,
            color='#cccccc',  # ì—°í•œ íšŒìƒ‰
            edgecolor='#888888',  # ì–´ë‘ìš´ í…Œë‘ë¦¬
            linewidth=1,
            label='ìœ ì‚¬ í”„ë¡œì íŠ¸ ë¶„í¬',
            align='mid'
        )

        # ì˜ˆì¸¡ê°’ ë¼ì¸
        ax.axvline(pred1, color='blue', linestyle='--', linewidth=1.2, label='ì„ í˜•íšŒê·€ ì˜ˆì¸¡')
        ax.axvline(pred2, color='green', linestyle='--', linewidth=1.2, label='ëœë¤í¬ë ˆìŠ¤íŠ¸ ì˜ˆì¸¡')
        ax.axvline(pred3, color='orange', linestyle='--', linewidth=1.2, label='ìœ ì‚¬ ê¸°ë°˜ ì˜ˆì¸¡')
        ax.axvline(pred_ensemble, color='red', linestyle='-', linewidth=2.5, label='ì•™ìƒë¸” ì˜ˆì¸¡')

        # ìŠ¤íƒ€ì¼ í–¥ìƒ
        ax.grid(axis='y', linestyle=':', linewidth=0.7, alpha=0.5)
        ax.set_facecolor('#f9f9f9')  # ë°ì€ íšŒìƒ‰ ë°°ê²½

        # í°íŠ¸ ë° ë ˆì´ì•„ì›ƒ
        ax.set_xticks(np.arange(min_month, max_month + 1, 1))
        ax.set_xlabel("ê³µì‚¬ê¸°ê°„", fontproperties=fontprop)
        ax.set_ylabel("ìœ ì‚¬ í”„ë¡œì íŠ¸ ìˆ˜", fontproperties=fontprop)
        ax.set_title("ê³µì‚¬ê¸°ê°„ ì˜ˆì¸¡ ê²°ê³¼ ë° ìœ ì‚¬ í”„ë¡œì íŠ¸ ë¶„í¬", fontproperties=fontprop)

        # ë²”ë¡€ í•˜ë‹¨ ê°€ë¡œ ì •ë ¬
        ax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.25), ncol=5, prop=fontprop, frameon=False)

        st.pyplot(fig)

    
    # ğŸ“‰ ì‹ ë¢°ë„ ê¸°ë°˜ ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥
    warnings = []

    # 1. ìœ ì‚¬ í”„ë¡œì íŠ¸ ê°„ ê³µì‚¬ê¸°ê°„ í‘œì¤€í¸ì°¨ê°€ ë„ˆë¬´ í¬ë©´ â†’ ì‹ ë¢° ë‚®ìŒ
    if sim_std > 6:
        warnings.append("ìœ ì‚¬ í”„ë¡œì íŠ¸ ê°„ ê³µì‚¬ê¸°ê°„ í¸ì°¨ê°€ ì»¤ì„œ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤.")

    # 2. ìœ ì‚¬ë„ í‰ê· ì´ ë‚®ìœ¼ë©´ â†’ ìœ ì‚¬ ì‚¬ë¡€ê°€ ê±°ì˜ ì—†ìŒ
    mean_similarity = similar_df["ìœ ì‚¬ë„ì ìˆ˜"].mean()
    if mean_similarity < 25:
        warnings.append("ì…ë ¥ ì¡°ê±´ê³¼ ìœ ì‚¬í•œ í”„ë¡œì íŠ¸ê°€ ì ì–´ ì˜ˆì¸¡ ì •í™•ë„ê°€ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # 3. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ RÂ² ì ìˆ˜ê°€ ë‘˜ ë‹¤ ë‚®ìœ¼ë©´ â†’ ì¼ë°˜í™” ì„±ëŠ¥ ë‚®ìŒ
    if r2_1 < 0.3 and r2_2 < 0.3:
        warnings.append("ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ì…ë ¥ê°’ì„ ë‹¤ì‹œ í™•ì¸í•´ ì£¼ì„¸ìš”.")

    # ì‹¤ì œ ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥
    if warnings:
        st.markdown("### âš ï¸ ì˜ˆì¸¡ ê²°ê³¼ ì£¼ì˜ì‚¬í•­")
        for w in warnings:
            st.warning(w)

    # ìœ ì‚¬ í”„ë¡œì íŠ¸ ë¦¬ìŠ¤íŠ¸
    st.subheader("â–  ì°¸ì¡°ëœ ìœ ì‚¬ í”„ë¡œì íŠ¸")
    if not similar_df.empty:
        display_df = similar_df.copy()

        def decode_types(row, prefix):
            cols = [col for col in row.index if col.startswith(prefix)]
            return " + ".join(col.replace(prefix, '') for col in cols if row[col] == 1)

        display_df["êµ¬ì¡°í˜•ì‹"] = display_df.apply(lambda row: decode_types(row, "êµ¬ì¡°í˜•ì‹_"), axis=1)
        display_df["ì°½ê³ ìœ í˜•"] = display_df.apply(lambda row: decode_types(row, "ì°½ê³ ìœ í˜•_"), axis=1)
        display_df["ì¸µìˆ˜"] = display_df.apply(
            lambda row: f"B{int(row['ì§€í•˜ì¸µìˆ˜']) if not pd.isna(row['ì§€í•˜ì¸µìˆ˜']) else 0} / "
                        f"{int(row['ì§€ìƒì¸µìˆ˜']) if not pd.isna(row['ì§€ìƒì¸µìˆ˜']) else 0}F",
            axis=1
        )

        display_df = display_df[[
            "í”„ë¡œì íŠ¸ëª…", "ì—°ë©´ì ", "ì¸µìˆ˜", "êµ¬ì¡°í˜•ì‹", "ì°½ê³ ìœ í˜•", "ì „ì²´ê³µì‚¬ê¸°ê°„"
        ]].rename(columns={
            "ì—°ë©´ì ": "ì—°ë©´ì  (ã¡)",
            "ì „ì²´ê³µì‚¬ê¸°ê°„": "ê³µì‚¬ê¸°ê°„ (ê°œì›”)"
        })

        st.dataframe(display_df)
        st.markdown("ğŸ“Œ **ì°¸ì¡°ëœ ìœ ì‚¬ í”„ë¡œì íŠ¸**ëŠ” ì…ë ¥ ì¡°ê±´ê³¼ ë²”ì£¼í˜• í•­ëª©ì´ ì¼ì¹˜í•œ ì‹¤ì œ ì‚¬ë¡€ë“¤ì…ë‹ˆë‹¤.")
    else:
        st.warning("âš ï¸ ìœ ì‚¬ í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì…ë ¥ê°’ì„ ë‹¤ì‹œ í™•ì¸í•´ ì£¼ì„¸ìš”.")
